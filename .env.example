# Repo Intel Configuration File
# Copy this file to .env and configure your settings

# ===== LLM CONFIGURATION =====
# Choose your preferred LLM provider: 'openai', 'anthropic', 'local', or leave empty for auto-select
LLM_PROVIDER=

# Maximum combined file size for LLM analysis (bytes)
LLM_DEFAULT_MAX_FILE_SIZE=250000

# ===== OPENAI CONFIGURATION =====
OPENAI_API_KEY=
OPENAI_ORGANIZATION=
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.3
OPENAI_MAX_RETRIES=3
OPENAI_TIMEOUT=90

# ===== ANTHROPIC CONFIGURATION =====
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-sonnet-20240229
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1/messages
ANTHROPIC_TIMEOUT=90

# ===== LOCAL LLM CONFIGURATION =====
# For Ollama, LM Studio, or other local LLM services
LOCAL_LLM_BASE_URL=http://localhost:11434
LOCAL_LLM_MODEL=codellama
LOCAL_LLM_TIMEOUT=120

# ===== GIT CONFIGURATION =====
GIT_DEFAULT_REPO_PATH=.

# ===== OUTPUT CONFIGURATION =====
OUTPUT_DEFAULT_DIR=repo_intel_output
OUTPUT_VERBOSE=false

# ===== MARKDOWN BUNDLE CONFIGURATION =====
MARKDOWN_DEFAULT_OUTPUT=bundle.md
# Comma-separated list of patterns to exclude
MARKDOWN_EXCLUDE_PATTERNS=.egg-info,.aws-sam,.git,__pycache__,__tests__,node_modules,venv,.env,.idea,.yarn,.vscode

# ===== AWS CONFIGURATION =====
AWS_REGION=
AWS_PROFILE=

# ===== GLUE CONFIGURATION =====
GLUE_DEFAULT_OUTPUT=glue_documentation.md
# Comma-separated lists
GLUE_EXCLUDE_DATABASES=
GLUE_EXCLUDE_TABLES=

# ===== ADVANCED SETTINGS =====
# Path to alternative environment file
# ENV_FILE=~/.config/repo-intel/.env